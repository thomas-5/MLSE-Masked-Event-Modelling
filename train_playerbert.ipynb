{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1942ee1",
   "metadata": {},
   "source": [
    "# PlayerBERT Training (Masked Event Modeling)\n",
    "\n",
    "This notebook:\n",
    "- Loads the pre-trained `EventEncoder`.\n",
    "- Builds per-player, per-match event sequences ordered by timestamp.\n",
    "- Trains a **PlayerBERT** model using **Masked Event Modeling** (MSE on masked event embeddings).\n",
    "- Keeps the EventEncoder frozen (for simplicity) but uses it as the embedding initializer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b22cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5ee54e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Data: /content/drive/MyDrive/MLSE/events360_v4.jsonl\n",
      "EventEncoder: /content/drive/MyDrive/MLSE/models/event_encoder_mam.pt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Paths (edit if needed)\n",
    "DATA_PATH = Path('/content/drive/MyDrive/MLSE/events360_v4.jsonl')\n",
    "EVENT_ENCODER_CKPT = Path('/content/drive/MyDrive/MLSE/models/event_encoder_mam.pt')\n",
    "PLAYERBERT_OUT = Path('/content/drive/MyDrive/MLSE/models/playerbert_mam.pt')\n",
    "PLAYERBERT_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "print('Data:', DATA_PATH.resolve())\n",
    "print('EventEncoder:', EVENT_ENCODER_CKPT.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49368d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust JSONL reader\n",
    "\n",
    "def iter_json_objects(fp):\n",
    "    decoder = json.JSONDecoder()\n",
    "    for line in fp:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        idx = 0\n",
    "        while idx < len(line):\n",
    "            obj, end = decoder.raw_decode(line, idx)\n",
    "            yield obj\n",
    "            idx = end\n",
    "            while idx < len(line) and line[idx].isspace():\n",
    "                idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2481b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EventEncoder components (must match train_event_encoder)\n",
    "\n",
    "class PlayerMLP(nn.Module):\n",
    "    def __init__(self, in_dim=6, hidden=64, out_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class SetEncoder(nn.Module):\n",
    "    def __init__(self, player_dim=6, hidden=64, out_dim=128):\n",
    "        super().__init__()\n",
    "        self.player_mlp = PlayerMLP(in_dim=player_dim, hidden=hidden, out_dim=out_dim)\n",
    "\n",
    "    def forward(self, freeze_frames, actor_locs, device):\n",
    "        batch_embeds = []\n",
    "        for ff, (ax, ay) in zip(freeze_frames, actor_locs):\n",
    "            if ff is None or (hasattr(ff, '__len__') and len(ff) == 0):\n",
    "                batch_embeds.append(torch.zeros(128, device=device))\n",
    "                continue\n",
    "            per_player = []\n",
    "            for p in ff:\n",
    "                loc = p.get('location')\n",
    "                if loc is None or len(loc) < 2:\n",
    "                    continue\n",
    "                dx = float(loc[0]) - ax\n",
    "                dy = float(loc[1]) - ay\n",
    "                dist = math.sqrt(dx*dx + dy*dy)\n",
    "                angle = math.atan2(dy, dx)\n",
    "                is_teammate = 1.0 if p.get('teammate', False) else 0.0\n",
    "                is_keeper = 1.0 if p.get('keeper', False) else 0.0\n",
    "                vec = torch.tensor([dx, dy, dist, angle, is_teammate, is_keeper], device=device)\n",
    "                per_player.append(vec)\n",
    "            if not per_player:\n",
    "                batch_embeds.append(torch.zeros(128, device=device))\n",
    "                continue\n",
    "            players = torch.stack(per_player, dim=0)\n",
    "            emb = self.player_mlp(players).mean(dim=0)\n",
    "            batch_embeds.append(emb)\n",
    "        return torch.stack(batch_embeds, dim=0)\n",
    "\n",
    "\n",
    "class EventTransformer(nn.Module):\n",
    "    def __init__(self, vocab_sizes, d_model=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.features = list(vocab_sizes.keys())\n",
    "        self.safe_names = [f\"f{i}\" for i in range(len(self.features))]\n",
    "        self.name_map = dict(zip(self.features, self.safe_names))\n",
    "        self.value_embeds = nn.ModuleDict({\n",
    "            self.name_map[f]: nn.Embedding(vocab_sizes[f], d_model) for f in self.features\n",
    "        })\n",
    "        self.feature_embeds = nn.Embedding(len(self.features), d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, feat_ids):\n",
    "        B, F = feat_ids.shape\n",
    "        tokens = []\n",
    "        for i, f in enumerate(self.features):\n",
    "            v = self.value_embeds[self.name_map[f]](feat_ids[:, i])\n",
    "            f_emb = self.feature_embeds(torch.tensor(i, device=feat_ids.device))\n",
    "            tokens.append(v + f_emb)\n",
    "        x = torch.stack(tokens, dim=1)\n",
    "        h = self.encoder(x)\n",
    "        z_event = h.mean(dim=1)\n",
    "        return z_event, h\n",
    "\n",
    "\n",
    "class EventEncoder(nn.Module):\n",
    "    def __init__(self, vocab_sizes):\n",
    "        super().__init__()\n",
    "        self.event_encoder = EventTransformer(vocab_sizes)\n",
    "        self.frame_encoder = SetEncoder()\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(128 * 2, 128),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, feat_ids, freeze_frames, actor_locs, device):\n",
    "        z_event, _ = self.event_encoder(feat_ids)\n",
    "        z_frame = self.frame_encoder(freeze_frames, actor_locs, device)\n",
    "        g = self.gate(torch.cat([z_event, z_frame], dim=-1))\n",
    "        z = g * z_event + (1 - g) * z_frame\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8810123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded EventEncoder with 72 features\n"
     ]
    }
   ],
   "source": [
    "# Load EventEncoder checkpoint\n",
    "\n",
    "ckpt = torch.load(EVENT_ENCODER_CKPT, map_location='cpu')\n",
    "feature_vocab = ckpt['feature_vocab']\n",
    "vocab_sizes = {k: len(v) for k, v in feature_vocab.items()}\n",
    "\n",
    "encoder = EventEncoder(vocab_sizes).to(device)\n",
    "encoder.load_state_dict(ckpt['event_encoder'])\n",
    "encoder.eval()\n",
    "\n",
    "print('Loaded EventEncoder with', len(feature_vocab), 'features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f187f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: build feature ids for a flattened event\n",
    "\n",
    "UNK_TOKEN = '[UNK]'\n",
    "\n",
    "FEATURE_LIST = list(feature_vocab.keys())\n",
    "\n",
    "\n",
    "def build_feat_ids(ev):\n",
    "    ids = []\n",
    "    for feat in FEATURE_LIST:\n",
    "        val = ev.get(feat, UNK_TOKEN)\n",
    "        if isinstance(val, bool):\n",
    "            val = str(val)\n",
    "        if val is None:\n",
    "            val = UNK_TOKEN\n",
    "        idx = feature_vocab[feat].get(val, 0)\n",
    "        # safety clamp\n",
    "        if idx >= len(feature_vocab[feat]):\n",
    "            idx = 0\n",
    "        ids.append(idx)\n",
    "    return torch.tensor(ids, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae436c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad features: []\n",
      "Total bad features: 0\n"
     ]
    }
   ],
   "source": [
    "# Debug check: verify feature id ranges\n",
    "\n",
    "max_ids = [0]*len(FEATURE_LIST)\n",
    "\n",
    "with DATA_PATH.open('r', encoding='utf-8') as f:\n",
    "    for ev in iter_json_objects(f):\n",
    "        ids = build_feat_ids(ev)\n",
    "        for i, v in enumerate(ids.tolist()):\n",
    "            if v > max_ids[i]:\n",
    "                max_ids[i] = v\n",
    "\n",
    "bad = []\n",
    "for i, feat in enumerate(FEATURE_LIST):\n",
    "    if max_ids[i] >= len(feature_vocab[feat]):\n",
    "        bad.append((feat, max_ids[i], len(feature_vocab[feat])))\n",
    "\n",
    "print('Bad features:', bad[:10])\n",
    "print('Total bad features:', len(bad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c24092e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 2976\n"
     ]
    }
   ],
   "source": [
    "# Build per-player, per-match sequences ordered by timestamp\n",
    "\n",
    "# Group events by (match_id, player_id)\n",
    "sequences = defaultdict(list)\n",
    "\n",
    "with DATA_PATH.open('r', encoding='utf-8') as f:\n",
    "    for ev in iter_json_objects(f):\n",
    "        match_id = ev.get('match_id')\n",
    "        player_id = ev.get('player.id')\n",
    "        if match_id is None or player_id is None:\n",
    "            continue\n",
    "        sequences[(match_id, player_id)].append(ev)\n",
    "\n",
    "# Sort events by period, minute, second, timestamp\n",
    "for key, events in sequences.items():\n",
    "    events.sort(key=lambda e: (\n",
    "        e.get('period', 0),\n",
    "        e.get('minute', 0),\n",
    "        e.get('second', 0),\n",
    "        e.get('timestamp', ''),\n",
    "        e.get('index', 0),\n",
    "    ))\n",
    "\n",
    "print('Total sequences:', len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17240971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with sliding windows\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, max_len=256, stride=128):\n",
    "        self.samples = []\n",
    "        for key, events in sequences.items():\n",
    "            if len(events) == 0:\n",
    "                continue\n",
    "            if len(events) <= max_len:\n",
    "                self.samples.append(events)\n",
    "            else:\n",
    "                for i in range(0, len(events) - max_len + 1, stride):\n",
    "                    self.samples.append(events[i:i+max_len])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb0b2c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function: encode events with frozen EventEncoder\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # filter out empty sequences\n",
    "    batch = [seq for seq in batch if len(seq) > 0]\n",
    "    if len(batch) == 0:\n",
    "        return None, None\n",
    "\n",
    "    seq_lens = [len(seq) for seq in batch]\n",
    "    max_len = max(seq_lens)\n",
    "\n",
    "    embeddings = []\n",
    "    for seq in batch:\n",
    "        feat_ids = torch.stack([build_feat_ids(ev) for ev in seq], dim=0)\n",
    "        freeze_frames = [ev.get('freeze_frame') or [] for ev in seq]\n",
    "        actor_locs = []\n",
    "        for ev in seq:\n",
    "            loc = ev.get('location')\n",
    "            if loc is None or len(loc) < 2:\n",
    "                actor_locs.append((0.0, 0.0))\n",
    "            else:\n",
    "                actor_locs.append((float(loc[0]), float(loc[1])))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = encoder(\n",
    "                feat_ids.to(device),\n",
    "                freeze_frames,\n",
    "                actor_locs,\n",
    "                device,\n",
    "            )\n",
    "        embeddings.append(z.cpu())\n",
    "\n",
    "    d = embeddings[0].shape[-1]\n",
    "    padded = torch.zeros(len(batch), max_len, d)\n",
    "    attn_mask = torch.zeros(len(batch), max_len, dtype=torch.bool)\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        L = emb.shape[0]\n",
    "        padded[i, :L, :] = emb\n",
    "        attn_mask[i, :L] = 1\n",
    "\n",
    "    return padded, attn_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9604c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PlayerBERT model\n",
    "\n",
    "class PlayerBERT(nn.Module):\n",
    "    def __init__(self, embed_dim=128, nhead=4, num_layers=2, max_len=512):\n",
    "        super().__init__()\n",
    "        self.pos_embed = nn.Embedding(max_len, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "    def forward(self, x, attn_mask, mask_positions=None):\n",
    "        # x: (B, T, D)\n",
    "        B, T, D = x.shape\n",
    "        pos = torch.arange(T, device=x.device).unsqueeze(0).repeat(B, 1)\n",
    "        x = x + self.pos_embed(pos)\n",
    "\n",
    "        if mask_positions is not None:\n",
    "            x = x.clone()\n",
    "            x[mask_positions] = self.mask_token\n",
    "\n",
    "        # src_key_padding_mask expects True for padding\n",
    "        src_key_padding_mask = ~attn_mask\n",
    "        h = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e050664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Event Modeling on sequences\n",
    "\n",
    "def mask_sequence(attn_mask, mask_prob=0.15):\n",
    "    # attn_mask: (B, T) True for real tokens\n",
    "    B, T = attn_mask.shape\n",
    "    mask_positions = torch.zeros(B, T, dtype=torch.bool)\n",
    "    for i in range(B):\n",
    "        for j in range(T):\n",
    "            if not attn_mask[i, j]:\n",
    "                continue\n",
    "            if random.random() < mask_prob:\n",
    "                mask_positions[i, j] = True\n",
    "    return mask_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "432b3ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 3.4717\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "seq_dataset = SequenceDataset(sequences, max_len=256, stride=128)\n",
    "loader = DataLoader(seq_dataset, batch_size=16, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "playerbert = PlayerBERT(embed_dim=128, nhead=4, num_layers=2, max_len=256).to(device)\n",
    "optimizer = torch.optim.Adam(playerbert.parameters(), lr=1e-4)\n",
    "\n",
    "playerbert.train()\n",
    "\n",
    "for epoch in range(1):\n",
    "    total_loss = 0.0\n",
    "    for batch_emb, attn_mask in loader:\n",
    "        if batch_emb is None:\n",
    "            continue\n",
    "        batch_emb = batch_emb.to(device)\n",
    "        attn_mask = attn_mask.to(device)\n",
    "\n",
    "        mask_pos = mask_sequence(attn_mask, mask_prob=0.15).to(device)\n",
    "        outputs = playerbert(batch_emb, attn_mask, mask_positions=mask_pos)\n",
    "\n",
    "        # MSE loss on masked positions\n",
    "        target = batch_emb\n",
    "        pred = outputs\n",
    "        loss = ((pred - target) ** 2)[mask_pos].mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} loss: {total_loss/len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb76db7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PlayerBERT to /content/drive/MyDrive/MLSE/models/playerbert_mam.pt\n"
     ]
    }
   ],
   "source": [
    "# Save PlayerBERT\n",
    "\n",
    "state = {\n",
    "    'playerbert': playerbert.state_dict(),\n",
    "}\n",
    "\n",
    "torch.save(state, PLAYERBERT_OUT)\n",
    "print('Saved PlayerBERT to', PLAYERBERT_OUT.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
