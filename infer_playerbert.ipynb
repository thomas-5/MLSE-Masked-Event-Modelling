{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78d8a8b2",
   "metadata": {},
   "source": [
    "# PlayerBERT Inference: Player Embeddings & Similarity\n",
    "\n",
    "This notebook loads saved weights and runs inference to build player embeddings and nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a23c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62153704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['events360_v4.jsonl', 'models']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('/content/drive/MyDrive/MLSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abc4acbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DATA_PATH = Path('/content/drive/MyDrive/MLSE/events360_v4.jsonl')\n",
    "EVENT_ENCODER_CKPT = Path('/content/drive/MyDrive/MLSE/models/event_encoder_mam.pt')\n",
    "PLAYERBERT_CKPT = Path('/content/drive/MyDrive/MLSE/models/playerbert_mam.pt')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37cde8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust JSONL reader\n",
    "\n",
    "def iter_json_objects(fp):\n",
    "    decoder = json.JSONDecoder()\n",
    "    for line in fp:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        idx = 0\n",
    "        while idx < len(line):\n",
    "            obj, end = decoder.raw_decode(line, idx)\n",
    "            yield obj\n",
    "            idx = end\n",
    "            while idx < len(line) and line[idx].isspace():\n",
    "                idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c72c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EventEncoder components (must match train_event_encoder)\n",
    "\n",
    "class PlayerMLP(nn.Module):\n",
    "    def __init__(self, in_dim=6, hidden=64, out_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SetEncoder(nn.Module):\n",
    "    def __init__(self, player_dim=6, hidden=64, out_dim=128):\n",
    "        super().__init__()\n",
    "        self.player_mlp = PlayerMLP(in_dim=player_dim, hidden=hidden, out_dim=out_dim)\n",
    "    def forward(self, freeze_frames, actor_locs, device):\n",
    "        batch_embeds = []\n",
    "        for ff, (ax, ay) in zip(freeze_frames, actor_locs):\n",
    "            if ff is None or (hasattr(ff, '__len__') and len(ff) == 0):\n",
    "                batch_embeds.append(torch.zeros(128, device=device))\n",
    "                continue\n",
    "            per_player = []\n",
    "            for p in ff:\n",
    "                loc = p.get('location')\n",
    "                if loc is None or len(loc) < 2:\n",
    "                    continue\n",
    "                dx = float(loc[0]) - ax\n",
    "                dy = float(loc[1]) - ay\n",
    "                dist = math.sqrt(dx*dx + dy*dy)\n",
    "                angle = math.atan2(dy, dx)\n",
    "                is_teammate = 1.0 if p.get('teammate', False) else 0.0\n",
    "                is_keeper = 1.0 if p.get('keeper', False) else 0.0\n",
    "                vec = torch.tensor([dx, dy, dist, angle, is_teammate, is_keeper], device=device)\n",
    "                per_player.append(vec)\n",
    "            if not per_player:\n",
    "                batch_embeds.append(torch.zeros(128, device=device))\n",
    "                continue\n",
    "            players = torch.stack(per_player, dim=0)\n",
    "            emb = self.player_mlp(players).mean(dim=0)\n",
    "            batch_embeds.append(emb)\n",
    "        return torch.stack(batch_embeds, dim=0)\n",
    "\n",
    "class EventTransformer(nn.Module):\n",
    "    def __init__(self, vocab_sizes, d_model=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.features = list(vocab_sizes.keys())\n",
    "        self.safe_names = [f\"f{i}\" for i in range(len(self.features))]\n",
    "        self.name_map = dict(zip(self.features, self.safe_names))\n",
    "        self.value_embeds = nn.ModuleDict({\n",
    "            self.name_map[f]: nn.Embedding(vocab_sizes[f], d_model) for f in self.features\n",
    "        })\n",
    "        self.feature_embeds = nn.Embedding(len(self.features), d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "    def forward(self, feat_ids):\n",
    "        B, F = feat_ids.shape\n",
    "        tokens = []\n",
    "        for i, f in enumerate(self.features):\n",
    "            v = self.value_embeds[self.name_map[f]](feat_ids[:, i])\n",
    "            f_emb = self.feature_embeds(torch.tensor(i, device=feat_ids.device))\n",
    "            tokens.append(v + f_emb)\n",
    "        x = torch.stack(tokens, dim=1)\n",
    "        h = self.encoder(x)\n",
    "        z_event = h.mean(dim=1)\n",
    "        return z_event\n",
    "\n",
    "class EventEncoder(nn.Module):\n",
    "    def __init__(self, vocab_sizes):\n",
    "        super().__init__()\n",
    "        self.event_encoder = EventTransformer(vocab_sizes)\n",
    "        self.frame_encoder = SetEncoder()\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(128 * 2, 128),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self, feat_ids, freeze_frames, actor_locs, device):\n",
    "        z_event = self.event_encoder(feat_ids)\n",
    "        z_frame = self.frame_encoder(freeze_frames, actor_locs, device)\n",
    "        g = self.gate(torch.cat([z_event, z_frame], dim=-1))\n",
    "        z = g * z_event + (1 - g) * z_frame\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd26f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PlayerBERT\n",
    "\n",
    "class PlayerBERT(nn.Module):\n",
    "    def __init__(self, embed_dim=128, nhead=4, num_layers=2, max_len=256):\n",
    "        super().__init__()\n",
    "        self.pos_embed = nn.Embedding(max_len, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(embed_dim))\n",
    "    def forward(self, x, attn_mask):\n",
    "        B, T, D = x.shape\n",
    "        pos = torch.arange(T, device=x.device).unsqueeze(0).repeat(B, 1)\n",
    "        x = x + self.pos_embed(pos)\n",
    "        src_key_padding_mask = ~attn_mask\n",
    "        h = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9927e5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded models\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoints\n",
    "\n",
    "ckpt = torch.load(EVENT_ENCODER_CKPT, map_location='cpu')\n",
    "feature_vocab = ckpt['feature_vocab']\n",
    "vocab_sizes = {k: len(v) for k, v in feature_vocab.items()}\n",
    "\n",
    "encoder = EventEncoder(vocab_sizes).to(device)\n",
    "encoder.load_state_dict(ckpt['event_encoder'])\n",
    "encoder.eval()\n",
    "\n",
    "playerbert = PlayerBERT(embed_dim=128, nhead=4, num_layers=2, max_len=256).to(device)\n",
    "playerbert.load_state_dict(torch.load(PLAYERBERT_CKPT, map_location='cpu')['playerbert'])\n",
    "playerbert.eval()\n",
    "\n",
    "print('Loaded models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff33fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: build feature ids for a flattened event\n",
    "\n",
    "UNK_TOKEN = '[UNK]'\n",
    "FEATURE_LIST = list(feature_vocab.keys())\n",
    "\n",
    "def build_feat_ids(ev):\n",
    "    ids = []\n",
    "    for feat in FEATURE_LIST:\n",
    "        val = ev.get(feat, UNK_TOKEN)\n",
    "        if isinstance(val, bool):\n",
    "            val = str(val)\n",
    "        if val is None:\n",
    "            val = UNK_TOKEN\n",
    "        idx = feature_vocab[feat].get(val, 0)\n",
    "        if idx >= len(feature_vocab[feat]):\n",
    "            idx = 0\n",
    "        ids.append(idx)\n",
    "    return torch.tensor(ids, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "314e85bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 2976\n"
     ]
    }
   ],
   "source": [
    "# Build per-player, per-match sequences ordered by timestamp\n",
    "\n",
    "sequences = defaultdict(list)\n",
    "with DATA_PATH.open('r', encoding='utf-8') as f:\n",
    "    for ev in iter_json_objects(f):\n",
    "        match_id = ev.get('match_id')\n",
    "        player_id = ev.get('player.id')\n",
    "        if match_id is None or player_id is None:\n",
    "            continue\n",
    "        sequences[(match_id, player_id)].append(ev)\n",
    "\n",
    "for key, events in sequences.items():\n",
    "    events.sort(key=lambda e: (\n",
    "        e.get('period', 0),\n",
    "        e.get('minute', 0),\n",
    "        e.get('second', 0),\n",
    "        e.get('timestamp', ''),\n",
    "        e.get('index', 0),\n",
    "    ))\n",
    "\n",
    "print('Total sequences:', len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7daca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build player embeddings and store on disk (run if needed)\n",
    "\n",
    "PLAYER_QUERY = 'Lionel Messi'  # not used in this cell\n",
    "TOP_K = 10  # not used in this cell\n",
    "MAX_LEN = 256\n",
    "STRIDE = 128\n",
    "\n",
    "player_embs = {}\n",
    "player_counts = {}\n",
    "player_names = {}\n",
    "\n",
    "for (match_id, player_id), events in sequences.items():\n",
    "    if len(events) == 0:\n",
    "        continue\n",
    "\n",
    "    windows = []\n",
    "    if len(events) <= MAX_LEN:\n",
    "        windows = [events]\n",
    "    else:\n",
    "        for i in range(0, len(events) - MAX_LEN + 1, STRIDE):\n",
    "            windows.append(events[i:i+MAX_LEN])\n",
    "\n",
    "    seq_embs = []\n",
    "    for win in windows:\n",
    "        feat_ids = torch.stack([build_feat_ids(ev) for ev in win], dim=0)\n",
    "        freeze_frames = [ev.get('freeze_frame') or [] for ev in win]\n",
    "        actor_locs = []\n",
    "        for ev in win:\n",
    "            loc = ev.get('location')\n",
    "            if loc is None or len(loc) < 2:\n",
    "                actor_locs.append((0.0, 0.0))\n",
    "            else:\n",
    "                actor_locs.append((float(loc[0]), float(loc[1])))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = encoder(feat_ids.to(device), freeze_frames, actor_locs, device)\n",
    "            z = z.unsqueeze(0)\n",
    "            attn_mask = torch.ones(1, z.shape[1], dtype=torch.bool, device=device)\n",
    "            out = playerbert(z, attn_mask)\n",
    "\n",
    "        seq_emb = out.mean(dim=1).squeeze(0).cpu()\n",
    "        seq_embs.append(seq_emb)\n",
    "\n",
    "    if not seq_embs:\n",
    "        continue\n",
    "\n",
    "    seq_emb = torch.stack(seq_embs, dim=0).mean(dim=0)\n",
    "\n",
    "    pname = events[0].get('player.name')\n",
    "    if pname:\n",
    "        player_names[player_id] = pname\n",
    "\n",
    "    if player_id not in player_embs:\n",
    "        player_embs[player_id] = seq_emb.clone()\n",
    "        player_counts[player_id] = 1\n",
    "    else:\n",
    "        player_embs[player_id] += seq_emb\n",
    "        player_counts[player_id] += 1\n",
    "\n",
    "for pid in list(player_embs.keys()):\n",
    "    player_embs[pid] = player_embs[pid] / player_counts[pid]\n",
    "\n",
    "# Save\n",
    "EMB_OUT = Path('/content/drive/MyDrive/MLSE/models/player_embeddings.pt')\n",
    "NAME_OUT = Path('/content/drive/MyDrive/MLSE/models/player_embeddings_names.json')\n",
    "\n",
    "player_ids = list(player_embs.keys())\n",
    "emb_matrix = torch.stack([player_embs[pid] for pid in player_ids], dim=0)\n",
    "\n",
    "torch.save({\n",
    "    'player_ids': player_ids,\n",
    "    'embeddings': emb_matrix,\n",
    "}, EMB_OUT)\n",
    "\n",
    "with NAME_OUT.open('w', encoding='utf-8') as f:\n",
    "    json.dump(player_names, f, ensure_ascii=False)\n",
    "\n",
    "print('Saved embeddings to', EMB_OUT.resolve())\n",
    "print('Saved names to', NAME_OUT.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ede2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load built embeddings (fast search)\n",
    "\n",
    "EMB_OUT = Path('/content/drive/MyDrive/MLSE/models/player_embeddings.pt')\n",
    "NAME_OUT = Path('/content/drive/MyDrive/MLSE/models/player_embeddings_names.json')\n",
    "\n",
    "emb_data = torch.load(EMB_OUT, map_location='cpu')\n",
    "player_ids = emb_data['player_ids']\n",
    "emb_matrix = emb_data['embeddings']\n",
    "\n",
    "with NAME_OUT.open('r', encoding='utf-8') as f:\n",
    "    player_names = json.load(f)\n",
    "\n",
    "print('Loaded embeddings:', emb_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e4eed1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 similar players to Wojciech Szczęsny:\n",
      "Robin Olsen 0.9992178678512573\n",
      "Thibaut Courtois 0.999181866645813\n",
      "Maarten Stekelenburg 0.9991317391395569\n",
      "Milan Borjan 0.9991282224655151\n",
      "Martin Dúbravka 0.9990377426147461\n",
      "Francisco Guillermo Ochoa Magaña 0.9990240931510925\n",
      "Lukáš Hrádecký 0.9990060329437256\n",
      "Matvey Safonov 0.9988555312156677\n",
      "Danny Ward 0.9987244009971619\n",
      "Unai Simón Mendibil 0.9986366033554077\n"
     ]
    }
   ],
   "source": [
    "# Similarity search interface\n",
    "\n",
    "PLAYER_QUERY = 'Wojciech Szczęsny'\n",
    "TOP_K = 10\n",
    "\n",
    "# map name -> id\n",
    "query_id = None\n",
    "for pid in player_ids:\n",
    "    name = player_names.get(str(pid)) or player_names.get(pid)\n",
    "    if name == PLAYER_QUERY:\n",
    "        query_id = pid\n",
    "        break\n",
    "\n",
    "if query_id is None:\n",
    "    print('Player not found:', PLAYER_QUERY)\n",
    "else:\n",
    "    # build id -> index\n",
    "    id_to_idx = {pid: i for i, pid in enumerate(player_ids)}\n",
    "    qidx = id_to_idx[query_id]\n",
    "    query_vec = emb_matrix[qidx]\n",
    "\n",
    "    # cosine similarity\n",
    "    query_vec = query_vec / (query_vec.norm() + 1e-8)\n",
    "    embs = emb_matrix / (emb_matrix.norm(dim=1, keepdim=True) + 1e-8)\n",
    "    sims = torch.mv(embs, query_vec)\n",
    "\n",
    "    # top-k\n",
    "    topk = torch.topk(sims, k=TOP_K + 1)\n",
    "    print(f\"Top {TOP_K} similar players to {PLAYER_QUERY}:\")\n",
    "    count = 0\n",
    "    for idx in topk.indices.tolist():\n",
    "        pid = player_ids[idx]\n",
    "        name = player_names.get(str(pid)) or player_names.get(pid)\n",
    "        if name == PLAYER_QUERY:\n",
    "            continue\n",
    "        print(name, float(sims[idx]))\n",
    "        count += 1\n",
    "        if count >= TOP_K:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1502c23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
